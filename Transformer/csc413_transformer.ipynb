{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x216dfc1feb0>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from attention import MultiHeadAttention, PositionalEncoding\n",
    "from components import PositionWiseFFN, AddNorm\n",
    "from encoder_decoder import Encoder, Decoder, EncoderDecoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "torch.manual_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:11.496710Z",
     "end_time": "2023-04-17T04:20:13.293176Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.294176Z",
     "end_time": "2023-04-17T04:20:13.352776Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.308208Z",
     "end_time": "2023-04-17T04:20:13.352776Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                 dropout, use_bias=False):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(key_size, query_size, value_size,\n",
    "                                            num_hiddens, num_heads, dropout, use_bias)\n",
    "        self.addNorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addNorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addNorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addNorm2(Y, self.ffn(Y))\n",
    "\n",
    "\n",
    "class TransformerEncoder(Encoder):\n",
    "    def __init__(self,\n",
    "                 num_data_dim,\n",
    "                 num_weather,\n",
    "                 weather_embedder_dim,\n",
    "                 num_hiddens,\n",
    "                 norm_shape,\n",
    "                 ffn_num_input,\n",
    "                 ffn_num_hiddens,\n",
    "                 num_heads,\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 use_bias=False,\n",
    "                 use_weather=True):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.use_weather = use_weather\n",
    "        self.num_hiddens = num_hiddens\n",
    "        if use_weather:\n",
    "            self._total_input_dim = num_data_dim + weather_embedder_dim\n",
    "            self.weather_embedder = nn.Embedding(num_weather, weather_embedder_dim)\n",
    "        else:\n",
    "            self._total_input_dim = num_data_dim\n",
    "        self.embedding = nn.Linear(self._total_input_dim, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(self.num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"encoder_blk_%d\" % i, EncoderBlock(num_hiddens,\n",
    "                                                                    num_hiddens,\n",
    "                                                                    num_hiddens,\n",
    "                                                                    num_hiddens,\n",
    "                                                                    norm_shape,\n",
    "                                                                    ffn_num_input,\n",
    "                                                                    ffn_num_hiddens,\n",
    "                                                                    num_heads,\n",
    "                                                                    dropout,\n",
    "                                                                    use_bias))\n",
    "\n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        if self.use_weather:\n",
    "            data, weather = X\n",
    "            weather = self.weather_embedder(weather)\n",
    "            X = torch.cat((data, weather), dim=-1)\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(10, 5, 10, 24, [24], 24, 48, 8, 2, 0.5)\n",
    "input_data = torch.randn(3, 10, 10)\n",
    "input_weather = torch.randint(0, 5, (3, 10))\n",
    "val_lens = torch.tensor([10, 10, 10])\n",
    "X = (input_data, input_weather)\n",
    "enc_outputs = encoder(X, val_lens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.324728Z",
     "end_time": "2023-04-17T04:20:13.444853Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 10, 24])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outputs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.432853Z",
     "end_time": "2023-04-17T04:20:13.462374Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                 dropout, i):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.i = i\n",
    "        self.attention1 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addNorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.attention2 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addNorm2 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addNorm3 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "\n",
    "        batch_size, num_steps, _ = X.shape\n",
    "        dec_valid_lens = torch.arange(1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n",
    "        X2 = self.attention1(X, X, X, dec_valid_lens)\n",
    "        Y = self.addNorm1(X, X2)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        Z = self.addNorm2(Y, Y2)\n",
    "        return self.addNorm3(Z, self.ffn(Z)), state\n",
    "\n",
    "\n",
    "class TransformerDecoder(Decoder):\n",
    "    def __init__(self, num_data_dim, num_weather, weather_embedder_dim,\n",
    "                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                 num_heads, num_layers, dropout,use_weather=True):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self._attention_weights = None\n",
    "        self.seqX = None\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.use_weather = use_weather\n",
    "        self.num_layers = num_layers\n",
    "        if use_weather:\n",
    "            self._total_input_dim = num_data_dim + weather_embedder_dim\n",
    "            self.weather_embedder = nn.Embedding(num_weather, weather_embedder_dim)\n",
    "        else:\n",
    "            self._total_input_dim = num_data_dim\n",
    "        self.pos_encoding = PositionalEncoding(self.num_hiddens, dropout)\n",
    "        self.embedding = nn.Linear(self._total_input_dim, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\" + str(i),\n",
    "                                 DecoderBlock(num_hiddens,\n",
    "                                              num_hiddens,\n",
    "                                              num_hiddens,\n",
    "                                              num_hiddens,\n",
    "                                              norm_shape,\n",
    "                                              ffn_num_input,\n",
    "                                              ffn_num_hiddens,\n",
    "                                              num_heads,\n",
    "                                              dropout,\n",
    "                                              i))\n",
    "        self.dense = nn.Linear(num_hiddens, num_data_dim+num_weather)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        self.seqX = None\n",
    "        return [enc_outputs, enc_valid_lens]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        if self.use_weather:\n",
    "            data, weather = X\n",
    "            weather = self.weather_embedder(weather)\n",
    "            X = torch.cat((data, weather), dim=-1)\n",
    "        if not self.training:\n",
    "            self.seqX = X if self.seqX is None else torch.cat((self.seqX, X), dim=1)\n",
    "            X = self.seqX\n",
    "\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "        if not self.training:\n",
    "            return self.dense(X)[:, -1:, :], state\n",
    "        return self.dense(X), state\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.447361Z",
     "end_time": "2023-04-17T04:20:13.462374Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(10, 5, 10, 24, [24], 24, 48, 8, 2, 0.5)\n",
    "target_data = torch.randn(3, 5, 10)\n",
    "target_weather = torch.randint(0, 5, (3, 5))\n",
    "target_val_lens = torch.tensor([5, 5, 5])\n",
    "target_X = (target_data, target_weather)\n",
    "state = decoder.init_state(enc_outputs, val_lens)\n",
    "dec_outputs, state = decoder(target_X, state)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.464887Z",
     "end_time": "2023-04-17T04:20:13.644077Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 5, 15])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_outputs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.632076Z",
     "end_time": "2023-04-17T04:20:13.660593Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 10, 24])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.648593Z",
     "end_time": "2023-04-17T04:20:13.675110Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "data_loss = nn.MSELoss()\n",
    "weather_loss = nn.CrossEntropyLoss()\n",
    "def combined_loss(pred, target_data, target_weather):\n",
    "    data = pred[:, :, :target_data.shape[-1]]\n",
    "    weather = pred[:, :, target_data.shape[-1]:]\n",
    "    data_loss = nn.MSELoss()(data, target_data)\n",
    "    weather_loss = nn.CrossEntropyLoss()(weather.permute(0, 2, 1), target_weather)\n",
    "    return data_loss + weather_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.665112Z",
     "end_time": "2023-04-17T04:20:13.722181Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2.8840, grad_fn=<AddBackward0>)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_loss(dec_outputs, target_data, target_weather)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.680618Z",
     "end_time": "2023-04-17T04:20:13.749211Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from util import Accumulator, grad_clipping\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def train_weather_transformer(model, num_epochs, data_iter, loss, device, lr, mile_stone, gamma):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=mile_stone, gamma=gamma)\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = Accumulator(2)\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X_data, X_weather, X_valid_len, Y_data, Y_weather, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos_data = X_data[:, -1, :]\n",
    "            bos_weather = X_weather[:, -1]\n",
    "            dec_input_data = torch.cat([bos_data.unsqueeze(1), Y_data[:, :-1, :]], 1)\n",
    "            dec_input_weather = torch.cat([bos_weather.unsqueeze(1), Y_weather[:, :-1]], 1)\n",
    "            dec_input = (dec_input_data, dec_input_weather)\n",
    "            X = (X_data, X_weather)\n",
    "            Y_hat, _ = model(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y_data, Y_weather)\n",
    "        #     X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "        #     bos = torch.tensor([target_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1)\n",
    "        #     dec_input = torch.cat([bos, Y[:, :-1]], 1)\n",
    "        #     Y_hat, _ = model(X, dec_input, X_valid_len)\n",
    "        #     print(Y_hat.shape)\n",
    "        #     l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()\n",
    "            grad_clipping(model, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        print('Epoch: {}/{}.............'.format(epoch+1, num_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format((metric[0] / metric[1])))\n",
    "        if (metric[0] / metric[1]) < best_loss:\n",
    "            best_loss = (metric[0] / metric[1])\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_weights, 'test_weather_predicting.pth')\n",
    "        scheduler.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.716183Z",
     "end_time": "2023-04-17T04:20:13.749211Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "def create_weather_dataset(file, input_len, output_len, batch_size=64):\n",
    "    all_data_and_weather = pd.read_csv(file).to_numpy()\n",
    "    used_data_and_weather = all_data_and_weather[:(len(all_data_and_weather)//(input_len+output_len)) * (input_len+output_len)]\n",
    "    data = used_data_and_weather[:, :-1]\n",
    "    weather = used_data_and_weather[:, -1]\n",
    "    data_split = np.split(data, len(data)//(input_len+output_len))\n",
    "    weather_split = np.split(weather, len(weather)//(input_len+output_len))\n",
    "    X_data = torch.tensor(np.stack([i[:input_len] for i in data_split]), dtype=torch.float)\n",
    "    X_weather = torch.tensor(np.stack([i[:input_len] for i in weather_split]), dtype=torch.long)\n",
    "    Y_data = torch.tensor(np.stack([i[input_len:] for i in data_split]), dtype=torch.float)\n",
    "    Y_weather = torch.tensor(np.stack([i[input_len:] for i in weather_split]), dtype=torch.long)\n",
    "    X_valid_len = torch.tensor([input_len] * len(X_data), dtype=torch.long)\n",
    "    Y_valid_len = torch.tensor([output_len] * len(Y_data), dtype=torch.long)\n",
    "    return load_array((X_data, X_weather, X_valid_len, Y_data, Y_weather, Y_valid_len), batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.729180Z",
     "end_time": "2023-04-17T04:20:13.749211Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iter = create_weather_dataset('train_data1.csv', 72, 24, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.741697Z",
     "end_time": "2023-04-17T04:20:13.829787Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "num_hiddens, num_layers, dropout= 64, 4, 0.2\n",
    "input_num_steps, target_num_steps = 72, 24\n",
    "lr, num_epochs = 0.0006, 1000\n",
    "ffn_num_input, ffn_num_hiddens, num_heads = 64, 256, 8\n",
    "num_data_dim, num_weather = 7, 27\n",
    "weather_embedder_dim = 16\n",
    "norm_shape = [num_hiddens]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.788247Z",
     "end_time": "2023-04-17T04:20:13.843306Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(num_data_dim,\n",
    "                             num_weather,\n",
    "                             weather_embedder_dim,\n",
    "                             num_hiddens,\n",
    "                             norm_shape,\n",
    "                             ffn_num_input,\n",
    "                             ffn_num_hiddens,\n",
    "                             num_heads,\n",
    "                             num_layers,\n",
    "                             dropout)\n",
    "\n",
    "decoder = TransformerDecoder(num_data_dim,\n",
    "                                num_weather,\n",
    "                                weather_embedder_dim,\n",
    "                                num_hiddens,\n",
    "                                norm_shape,\n",
    "                                ffn_num_input,\n",
    "                                ffn_num_hiddens,\n",
    "                                num_heads,\n",
    "                                num_layers,\n",
    "                                dropout)\n",
    "\n",
    "model = EncoderDecoder(encoder, decoder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:20:13.801271Z",
     "end_time": "2023-04-17T04:20:13.844306Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000............. Loss: 1.6775\n",
      "Epoch: 2/1000............. Loss: 1.6342\n",
      "Epoch: 3/1000............. Loss: 1.6444\n",
      "Epoch: 4/1000............. Loss: 1.5924\n",
      "Epoch: 5/1000............. Loss: 1.5743\n",
      "Epoch: 6/1000............. Loss: 1.5631\n",
      "Epoch: 7/1000............. Loss: 1.5715\n",
      "Epoch: 8/1000............. Loss: 1.5543\n",
      "Epoch: 9/1000............. Loss: 1.5409\n",
      "Epoch: 10/1000............. Loss: 1.5036\n",
      "Epoch: 11/1000............. Loss: 1.4780\n",
      "Epoch: 12/1000............. Loss: 1.4739\n",
      "Epoch: 13/1000............. Loss: 1.4767\n",
      "Epoch: 14/1000............. Loss: 1.4285\n",
      "Epoch: 15/1000............. Loss: 1.4280\n",
      "Epoch: 16/1000............. Loss: 1.4060\n",
      "Epoch: 17/1000............. Loss: 1.3805\n",
      "Epoch: 18/1000............. Loss: 1.3817\n",
      "Epoch: 19/1000............. Loss: 1.3700\n",
      "Epoch: 20/1000............. Loss: 1.3589\n",
      "Epoch: 21/1000............. Loss: 1.3495\n",
      "Epoch: 22/1000............. Loss: 1.2917\n",
      "Epoch: 23/1000............. Loss: 1.2702\n",
      "Epoch: 24/1000............. Loss: 1.2600\n",
      "Epoch: 25/1000............. Loss: 1.2493\n",
      "Epoch: 26/1000............. Loss: 1.2463\n",
      "Epoch: 27/1000............. Loss: 1.1967\n",
      "Epoch: 28/1000............. Loss: 1.2013\n",
      "Epoch: 29/1000............. Loss: 1.1686\n",
      "Epoch: 30/1000............. Loss: 1.1526\n",
      "Epoch: 31/1000............. Loss: 1.1395\n",
      "Epoch: 32/1000............. Loss: 1.1131\n",
      "Epoch: 33/1000............. Loss: 1.0941\n",
      "Epoch: 34/1000............. Loss: 1.0667\n",
      "Epoch: 35/1000............. Loss: 1.0795\n",
      "Epoch: 36/1000............. Loss: 1.0500\n",
      "Epoch: 37/1000............. Loss: 1.0258\n",
      "Epoch: 38/1000............. Loss: 1.0203\n",
      "Epoch: 39/1000............. Loss: 0.9731\n",
      "Epoch: 40/1000............. Loss: 0.9847\n",
      "Epoch: 41/1000............. Loss: 0.9235\n",
      "Epoch: 42/1000............. Loss: 0.9224\n",
      "Epoch: 43/1000............. Loss: 0.9229\n",
      "Epoch: 44/1000............. Loss: 0.8988\n",
      "Epoch: 45/1000............. Loss: 0.8689\n",
      "Epoch: 46/1000............. Loss: 0.8526\n",
      "Epoch: 47/1000............. Loss: 0.8354\n",
      "Epoch: 48/1000............. Loss: 0.8192\n",
      "Epoch: 49/1000............. Loss: 0.8064\n",
      "Epoch: 50/1000............. Loss: 0.7885\n",
      "Epoch: 51/1000............. Loss: 0.7663\n",
      "Epoch: 52/1000............. Loss: 0.7558\n",
      "Epoch: 53/1000............. Loss: 0.7319\n",
      "Epoch: 54/1000............. Loss: 0.7166\n",
      "Epoch: 55/1000............. Loss: 0.6860\n",
      "Epoch: 56/1000............. Loss: 0.6875\n",
      "Epoch: 57/1000............. Loss: 0.6676\n",
      "Epoch: 58/1000............. Loss: 0.6589\n",
      "Epoch: 59/1000............. Loss: 0.6591\n",
      "Epoch: 60/1000............. Loss: 0.6275\n",
      "Epoch: 61/1000............. Loss: 0.5909\n",
      "Epoch: 62/1000............. Loss: 0.5939\n",
      "Epoch: 63/1000............. Loss: 0.5794\n",
      "Epoch: 64/1000............. Loss: 0.5526\n",
      "Epoch: 65/1000............. Loss: 0.5329\n",
      "Epoch: 66/1000............. Loss: 0.5150\n",
      "Epoch: 67/1000............. Loss: 0.5012\n",
      "Epoch: 68/1000............. Loss: 0.4949\n",
      "Epoch: 69/1000............. Loss: 0.4927\n",
      "Epoch: 70/1000............. Loss: 0.4663\n",
      "Epoch: 71/1000............. Loss: 0.4440\n",
      "Epoch: 72/1000............. Loss: 0.4356\n",
      "Epoch: 73/1000............. Loss: 0.4164\n",
      "Epoch: 74/1000............. Loss: 0.4121\n",
      "Epoch: 75/1000............. Loss: 0.3899\n",
      "Epoch: 76/1000............. Loss: 0.3753\n",
      "Epoch: 77/1000............. Loss: 0.3721\n",
      "Epoch: 78/1000............. Loss: 0.3476\n",
      "Epoch: 79/1000............. Loss: 0.3464\n",
      "Epoch: 80/1000............. Loss: 0.3184\n",
      "Epoch: 81/1000............. Loss: 0.3192\n",
      "Epoch: 82/1000............. Loss: 0.3047\n",
      "Epoch: 83/1000............. Loss: 0.3040\n",
      "Epoch: 84/1000............. Loss: 0.2791\n",
      "Epoch: 85/1000............. Loss: 0.2630\n",
      "Epoch: 86/1000............. Loss: 0.2518\n",
      "Epoch: 87/1000............. Loss: 0.2414\n",
      "Epoch: 88/1000............. Loss: 0.2355\n",
      "Epoch: 89/1000............. Loss: 0.2268\n",
      "Epoch: 90/1000............. Loss: 0.2095\n",
      "Epoch: 91/1000............. Loss: 0.2051\n",
      "Epoch: 92/1000............. Loss: 0.1915\n",
      "Epoch: 93/1000............. Loss: 0.1913\n",
      "Epoch: 94/1000............. Loss: 0.1739\n",
      "Epoch: 95/1000............. Loss: 0.1653\n",
      "Epoch: 96/1000............. Loss: 0.1514\n",
      "Epoch: 97/1000............. Loss: 0.1495\n",
      "Epoch: 98/1000............. Loss: 0.1401\n",
      "Epoch: 99/1000............. Loss: 0.1320\n",
      "Epoch: 100/1000............. Loss: 0.1252\n",
      "Epoch: 101/1000............. Loss: 0.1230\n",
      "Epoch: 102/1000............. Loss: 0.1151\n",
      "Epoch: 103/1000............. Loss: 0.1027\n",
      "Epoch: 104/1000............. Loss: 0.0990\n",
      "Epoch: 105/1000............. Loss: 0.0905\n",
      "Epoch: 106/1000............. Loss: 0.0880\n",
      "Epoch: 107/1000............. Loss: 0.0805\n",
      "Epoch: 108/1000............. Loss: 0.0783\n",
      "Epoch: 109/1000............. Loss: 0.0734\n",
      "Epoch: 110/1000............. Loss: 0.0690\n",
      "Epoch: 111/1000............. Loss: 0.0682\n",
      "Epoch: 112/1000............. Loss: 0.0600\n",
      "Epoch: 113/1000............. Loss: 0.0570\n",
      "Epoch: 114/1000............. Loss: 0.0533\n",
      "Epoch: 115/1000............. Loss: 0.0527\n",
      "Epoch: 116/1000............. Loss: 0.0477\n",
      "Epoch: 117/1000............. Loss: 0.0496\n",
      "Epoch: 118/1000............. Loss: 0.0426\n",
      "Epoch: 119/1000............. Loss: 0.0417\n",
      "Epoch: 120/1000............. Loss: 0.0419\n",
      "Epoch: 121/1000............. Loss: 0.0368\n",
      "Epoch: 122/1000............. Loss: 0.0337\n",
      "Epoch: 123/1000............. Loss: 0.0322\n",
      "Epoch: 124/1000............. Loss: 0.0305\n",
      "Epoch: 125/1000............. Loss: 0.0293\n",
      "Epoch: 126/1000............. Loss: 0.0294\n",
      "Epoch: 127/1000............. Loss: 0.0258\n",
      "Epoch: 128/1000............. Loss: 0.0257\n",
      "Epoch: 129/1000............. Loss: 0.0260\n",
      "Epoch: 130/1000............. Loss: 0.0231\n",
      "Epoch: 131/1000............. Loss: 0.0237\n",
      "Epoch: 132/1000............. Loss: 0.0222\n",
      "Epoch: 133/1000............. Loss: 0.0238\n",
      "Epoch: 134/1000............. Loss: 0.0216\n",
      "Epoch: 135/1000............. Loss: 0.0225\n",
      "Epoch: 136/1000............. Loss: 0.0225\n",
      "Epoch: 137/1000............. Loss: 0.0197\n",
      "Epoch: 138/1000............. Loss: 0.0211\n",
      "Epoch: 139/1000............. Loss: 0.0204\n",
      "Epoch: 140/1000............. Loss: 0.0175\n",
      "Epoch: 141/1000............. Loss: 0.0168\n",
      "Epoch: 142/1000............. Loss: 0.0165\n",
      "Epoch: 143/1000............. Loss: 0.0173\n",
      "Epoch: 144/1000............. Loss: 0.0171\n",
      "Epoch: 145/1000............. Loss: 0.0169\n",
      "Epoch: 146/1000............. Loss: 0.0158\n",
      "Epoch: 147/1000............. Loss: 0.0153\n",
      "Epoch: 148/1000............. Loss: 0.0158\n",
      "Epoch: 149/1000............. Loss: 0.0159\n",
      "Epoch: 150/1000............. Loss: 0.0151\n",
      "Epoch: 151/1000............. Loss: 0.0156\n",
      "Epoch: 152/1000............. Loss: 0.0168\n",
      "Epoch: 153/1000............. Loss: 0.0152\n",
      "Epoch: 154/1000............. Loss: 0.0143\n",
      "Epoch: 155/1000............. Loss: 0.0133\n",
      "Epoch: 156/1000............. Loss: 0.0129\n",
      "Epoch: 157/1000............. Loss: 0.0135\n",
      "Epoch: 158/1000............. Loss: 0.0135\n",
      "Epoch: 159/1000............. Loss: 0.0137\n",
      "Epoch: 160/1000............. Loss: 0.0122\n",
      "Epoch: 161/1000............. Loss: 0.0127\n",
      "Epoch: 162/1000............. Loss: 0.0123\n",
      "Epoch: 163/1000............. Loss: 0.0127\n",
      "Epoch: 164/1000............. Loss: 0.0120\n",
      "Epoch: 165/1000............. Loss: 0.0119\n",
      "Epoch: 166/1000............. Loss: 0.0111\n",
      "Epoch: 167/1000............. Loss: 0.0113\n",
      "Epoch: 168/1000............. Loss: 0.0109\n",
      "Epoch: 169/1000............. Loss: 0.0105\n",
      "Epoch: 170/1000............. Loss: 0.0111\n",
      "Epoch: 171/1000............. Loss: 0.0104\n",
      "Epoch: 172/1000............. Loss: 0.0112\n",
      "Epoch: 173/1000............. Loss: 0.0104\n",
      "Epoch: 174/1000............. Loss: 0.0106\n",
      "Epoch: 175/1000............. Loss: 0.0105\n",
      "Epoch: 176/1000............. Loss: 0.0107\n",
      "Epoch: 177/1000............. Loss: 0.0099\n",
      "Epoch: 178/1000............. Loss: 0.0100\n",
      "Epoch: 179/1000............. Loss: 0.0105\n",
      "Epoch: 180/1000............. Loss: 0.0103\n",
      "Epoch: 181/1000............. Loss: 0.0091\n",
      "Epoch: 182/1000............. Loss: 0.0102\n",
      "Epoch: 183/1000............. Loss: 0.0096\n",
      "Epoch: 184/1000............. Loss: 0.0094\n",
      "Epoch: 185/1000............. Loss: 0.0092\n",
      "Epoch: 186/1000............. Loss: 0.0098\n",
      "Epoch: 187/1000............. Loss: 0.0100\n",
      "Epoch: 188/1000............. Loss: 0.0093\n",
      "Epoch: 189/1000............. Loss: 0.0091\n",
      "Epoch: 190/1000............. Loss: 0.0096\n",
      "Epoch: 191/1000............. Loss: 0.0092\n",
      "Epoch: 192/1000............. Loss: 0.0099\n",
      "Epoch: 193/1000............. Loss: 0.0088\n",
      "Epoch: 194/1000............. Loss: 0.0088\n",
      "Epoch: 195/1000............. Loss: 0.0091\n",
      "Epoch: 196/1000............. Loss: 0.0089\n",
      "Epoch: 197/1000............. Loss: 0.0086\n",
      "Epoch: 198/1000............. Loss: 0.0088\n",
      "Epoch: 199/1000............. Loss: 0.0094\n",
      "Epoch: 200/1000............. Loss: 0.0086\n",
      "Epoch: 201/1000............. Loss: 0.0083\n",
      "Epoch: 202/1000............. Loss: 0.0084\n",
      "Epoch: 203/1000............. Loss: 0.0090\n",
      "Epoch: 204/1000............. Loss: 0.0087\n",
      "Epoch: 205/1000............. Loss: 0.0083\n",
      "Epoch: 206/1000............. Loss: 0.0085\n",
      "Epoch: 207/1000............. Loss: 0.0090\n",
      "Epoch: 208/1000............. Loss: 0.0087\n",
      "Epoch: 209/1000............. Loss: 0.0085\n",
      "Epoch: 210/1000............. Loss: 0.0087\n",
      "Epoch: 211/1000............. Loss: 0.0085\n",
      "Epoch: 212/1000............. Loss: 0.0085\n",
      "Epoch: 213/1000............. Loss: 0.0086\n",
      "Epoch: 214/1000............. Loss: 0.0084\n",
      "Epoch: 215/1000............. Loss: 0.0083\n",
      "Epoch: 216/1000............. Loss: 0.0079\n",
      "Epoch: 217/1000............. Loss: 0.0082\n",
      "Epoch: 218/1000............. Loss: 0.0084\n",
      "Epoch: 219/1000............. Loss: 0.0082\n",
      "Epoch: 220/1000............. Loss: 0.0079\n",
      "Epoch: 221/1000............. Loss: 0.0080\n",
      "Epoch: 222/1000............. Loss: 0.0085\n",
      "Epoch: 223/1000............. Loss: 0.0079\n",
      "Epoch: 224/1000............. Loss: 0.0082\n",
      "Epoch: 225/1000............. Loss: 0.0081\n",
      "Epoch: 226/1000............. Loss: 0.0080\n",
      "Epoch: 227/1000............. Loss: 0.0078\n",
      "Epoch: 228/1000............. Loss: 0.0078\n",
      "Epoch: 229/1000............. Loss: 0.0081\n",
      "Epoch: 230/1000............. Loss: 0.0078\n",
      "Epoch: 231/1000............. Loss: 0.0079\n",
      "Epoch: 232/1000............. Loss: 0.0079\n",
      "Epoch: 233/1000............. Loss: 0.0077\n",
      "Epoch: 234/1000............. Loss: 0.0086\n",
      "Epoch: 235/1000............. Loss: 0.0081\n",
      "Epoch: 236/1000............. Loss: 0.0080\n",
      "Epoch: 237/1000............. Loss: 0.0077\n",
      "Epoch: 238/1000............. Loss: 0.0081\n",
      "Epoch: 239/1000............. Loss: 0.0082\n",
      "Epoch: 240/1000............. Loss: 0.0076\n",
      "Epoch: 241/1000............. Loss: 0.0079\n",
      "Epoch: 242/1000............. Loss: 0.0080\n",
      "Epoch: 243/1000............. Loss: 0.0077\n",
      "Epoch: 244/1000............. Loss: 0.0075\n",
      "Epoch: 245/1000............. Loss: 0.0081\n",
      "Epoch: 246/1000............. Loss: 0.0076\n",
      "Epoch: 247/1000............. Loss: 0.0083\n",
      "Epoch: 248/1000............. Loss: 0.0078\n",
      "Epoch: 249/1000............. Loss: 0.0078\n",
      "Epoch: 250/1000............. Loss: 0.0078\n",
      "Epoch: 251/1000............. Loss: 0.0080\n",
      "Epoch: 252/1000............. Loss: 0.0076\n",
      "Epoch: 253/1000............. Loss: 0.0075\n",
      "Epoch: 254/1000............. Loss: 0.0081\n",
      "Epoch: 255/1000............. Loss: 0.0079\n",
      "Epoch: 256/1000............. Loss: 0.0078\n",
      "Epoch: 257/1000............. Loss: 0.0076\n",
      "Epoch: 258/1000............. Loss: 0.0076\n",
      "Epoch: 259/1000............. Loss: 0.0073\n",
      "Epoch: 260/1000............. Loss: 0.0075\n",
      "Epoch: 261/1000............. Loss: 0.0071\n",
      "Epoch: 262/1000............. Loss: 0.0074\n",
      "Epoch: 263/1000............. Loss: 0.0078\n",
      "Epoch: 264/1000............. Loss: 0.0080\n",
      "Epoch: 265/1000............. Loss: 0.0074\n",
      "Epoch: 266/1000............. Loss: 0.0078\n",
      "Epoch: 267/1000............. Loss: 0.0074\n",
      "Epoch: 268/1000............. Loss: 0.0072\n",
      "Epoch: 269/1000............. Loss: 0.0083\n",
      "Epoch: 270/1000............. Loss: 0.0072\n",
      "Epoch: 271/1000............. Loss: 0.0079\n",
      "Epoch: 272/1000............. Loss: 0.0075\n",
      "Epoch: 273/1000............. Loss: 0.0072\n",
      "Epoch: 274/1000............. Loss: 0.0076\n",
      "Epoch: 275/1000............. Loss: 0.0078\n",
      "Epoch: 276/1000............. Loss: 0.0075\n",
      "Epoch: 277/1000............. Loss: 0.0074\n",
      "Epoch: 278/1000............. Loss: 0.0074\n",
      "Epoch: 279/1000............. Loss: 0.0072\n",
      "Epoch: 280/1000............. Loss: 0.0074\n",
      "Epoch: 281/1000............. Loss: 0.0075\n",
      "Epoch: 282/1000............. Loss: 0.0072\n",
      "Epoch: 283/1000............. Loss: 0.0072\n",
      "Epoch: 284/1000............. Loss: 0.0075\n",
      "Epoch: 285/1000............. Loss: 0.0071\n",
      "Epoch: 286/1000............. Loss: 0.0073\n",
      "Epoch: 287/1000............. Loss: 0.0071\n",
      "Epoch: 288/1000............. Loss: 0.0071\n",
      "Epoch: 289/1000............. Loss: 0.0077\n",
      "Epoch: 290/1000............. Loss: 0.0072\n",
      "Epoch: 291/1000............. Loss: 0.0075\n",
      "Epoch: 292/1000............. Loss: 0.0068\n",
      "Epoch: 293/1000............. Loss: 0.0073\n",
      "Epoch: 294/1000............. Loss: 0.0074\n",
      "Epoch: 295/1000............. Loss: 0.0074\n",
      "Epoch: 296/1000............. Loss: 0.0073\n",
      "Epoch: 297/1000............. Loss: 0.0077\n",
      "Epoch: 298/1000............. Loss: 0.0073\n",
      "Epoch: 299/1000............. Loss: 0.0078\n",
      "Epoch: 300/1000............. Loss: 0.0073\n",
      "Epoch: 301/1000............. Loss: 0.0073\n",
      "Epoch: 302/1000............. Loss: 0.0069\n",
      "Epoch: 303/1000............. Loss: 0.0070\n",
      "Epoch: 304/1000............. Loss: 0.0075\n",
      "Epoch: 305/1000............. Loss: 0.0071\n",
      "Epoch: 306/1000............. Loss: 0.0076\n",
      "Epoch: 307/1000............. Loss: 0.0074\n",
      "Epoch: 308/1000............. Loss: 0.0075\n",
      "Epoch: 309/1000............. Loss: 0.0075\n",
      "Epoch: 310/1000............. Loss: 0.0074\n",
      "Epoch: 311/1000............. Loss: 0.0073\n",
      "Epoch: 312/1000............. Loss: 0.0072\n",
      "Epoch: 313/1000............. Loss: 0.0078\n",
      "Epoch: 314/1000............. Loss: 0.0072\n",
      "Epoch: 315/1000............. Loss: 0.0076\n",
      "Epoch: 316/1000............. Loss: 0.0074\n",
      "Epoch: 317/1000............. Loss: 0.0070\n",
      "Epoch: 318/1000............. Loss: 0.0068\n",
      "Epoch: 319/1000............. Loss: 0.0072\n",
      "Epoch: 320/1000............. Loss: 0.0072\n",
      "Epoch: 321/1000............. Loss: 0.0071\n",
      "Epoch: 322/1000............. Loss: 0.0071\n",
      "Epoch: 323/1000............. Loss: 0.0072\n",
      "Epoch: 324/1000............. Loss: 0.0074\n",
      "Epoch: 325/1000............. Loss: 0.0072\n",
      "Epoch: 326/1000............. Loss: 0.0072\n",
      "Epoch: 327/1000............. Loss: 0.0073\n",
      "Epoch: 328/1000............. Loss: 0.0072\n",
      "Epoch: 329/1000............. Loss: 0.0068\n",
      "Epoch: 330/1000............. Loss: 0.0068\n",
      "Epoch: 331/1000............. Loss: 0.0069\n",
      "Epoch: 332/1000............. Loss: 0.0072\n",
      "Epoch: 333/1000............. Loss: 0.0071\n",
      "Epoch: 334/1000............. Loss: 0.0070\n",
      "Epoch: 335/1000............. Loss: 0.0073\n",
      "Epoch: 336/1000............. Loss: 0.0069\n",
      "Epoch: 337/1000............. Loss: 0.0072\n",
      "Epoch: 338/1000............. Loss: 0.0068\n",
      "Epoch: 339/1000............. Loss: 0.0073\n",
      "Epoch: 340/1000............. Loss: 0.0071\n",
      "Epoch: 341/1000............. Loss: 0.0072\n",
      "Epoch: 342/1000............. Loss: 0.0074\n",
      "Epoch: 343/1000............. Loss: 0.0070\n",
      "Epoch: 344/1000............. Loss: 0.0072\n",
      "Epoch: 345/1000............. Loss: 0.0071\n",
      "Epoch: 346/1000............. Loss: 0.0073\n",
      "Epoch: 347/1000............. Loss: 0.0073\n",
      "Epoch: 348/1000............. Loss: 0.0067\n",
      "Epoch: 349/1000............. Loss: 0.0069\n",
      "Epoch: 350/1000............. Loss: 0.0073\n",
      "Epoch: 351/1000............. Loss: 0.0070\n",
      "Epoch: 352/1000............. Loss: 0.0074\n",
      "Epoch: 353/1000............. Loss: 0.0071\n",
      "Epoch: 354/1000............. Loss: 0.0071\n",
      "Epoch: 355/1000............. Loss: 0.0070\n",
      "Epoch: 356/1000............. Loss: 0.0068\n",
      "Epoch: 357/1000............. Loss: 0.0069\n",
      "Epoch: 358/1000............. Loss: 0.0068\n",
      "Epoch: 359/1000............. Loss: 0.0069\n",
      "Epoch: 360/1000............. Loss: 0.0070\n",
      "Epoch: 361/1000............. Loss: 0.0070\n",
      "Epoch: 362/1000............. Loss: 0.0068\n",
      "Epoch: 363/1000............. Loss: 0.0071\n",
      "Epoch: 364/1000............. Loss: 0.0071\n",
      "Epoch: 365/1000............. Loss: 0.0071\n",
      "Epoch: 366/1000............. Loss: 0.0066\n",
      "Epoch: 367/1000............. Loss: 0.0072\n",
      "Epoch: 368/1000............. Loss: 0.0073\n",
      "Epoch: 369/1000............. Loss: 0.0067\n",
      "Epoch: 370/1000............. Loss: 0.0075\n",
      "Epoch: 371/1000............. Loss: 0.0068\n",
      "Epoch: 372/1000............. Loss: 0.0070\n",
      "Epoch: 373/1000............. Loss: 0.0070\n",
      "Epoch: 374/1000............. Loss: 0.0071\n",
      "Epoch: 375/1000............. Loss: 0.0071\n",
      "Epoch: 376/1000............. Loss: 0.0072\n",
      "Epoch: 377/1000............. Loss: 0.0071\n",
      "Epoch: 378/1000............. Loss: 0.0067\n",
      "Epoch: 379/1000............. Loss: 0.0070\n",
      "Epoch: 380/1000............. Loss: 0.0068\n",
      "Epoch: 381/1000............. Loss: 0.0066\n",
      "Epoch: 382/1000............. Loss: 0.0067\n",
      "Epoch: 383/1000............. Loss: 0.0073\n",
      "Epoch: 384/1000............. Loss: 0.0067\n",
      "Epoch: 385/1000............. Loss: 0.0069\n",
      "Epoch: 386/1000............. Loss: 0.0077\n",
      "Epoch: 387/1000............. Loss: 0.0069\n",
      "Epoch: 388/1000............. Loss: 0.0066\n",
      "Epoch: 389/1000............. Loss: 0.0073\n",
      "Epoch: 390/1000............. Loss: 0.0079\n",
      "Epoch: 391/1000............. Loss: 0.0068\n",
      "Epoch: 392/1000............. Loss: 0.0070\n",
      "Epoch: 393/1000............. Loss: 0.0065\n",
      "Epoch: 394/1000............. Loss: 0.0073\n",
      "Epoch: 395/1000............. Loss: 0.0073\n",
      "Epoch: 396/1000............. Loss: 0.0067\n",
      "Epoch: 397/1000............. Loss: 0.0067\n",
      "Epoch: 398/1000............. Loss: 0.0074\n",
      "Epoch: 399/1000............. Loss: 0.0066\n",
      "Epoch: 400/1000............. Loss: 0.0065\n",
      "Epoch: 401/1000............. Loss: 0.0070\n",
      "Epoch: 402/1000............. Loss: 0.0072\n",
      "Epoch: 403/1000............. Loss: 0.0066\n",
      "Epoch: 404/1000............. Loss: 0.0072\n",
      "Epoch: 405/1000............. Loss: 0.0071\n",
      "Epoch: 406/1000............. Loss: 0.0067\n",
      "Epoch: 407/1000............. Loss: 0.0071\n",
      "Epoch: 408/1000............. Loss: 0.0070\n",
      "Epoch: 409/1000............. Loss: 0.0073\n",
      "Epoch: 410/1000............. Loss: 0.0068\n",
      "Epoch: 411/1000............. Loss: 0.0070\n",
      "Epoch: 412/1000............. Loss: 0.0069\n",
      "Epoch: 413/1000............. Loss: 0.0067\n",
      "Epoch: 414/1000............. Loss: 0.0069\n",
      "Epoch: 415/1000............. Loss: 0.0065\n",
      "Epoch: 416/1000............. Loss: 0.0068\n",
      "Epoch: 417/1000............. Loss: 0.0069\n",
      "Epoch: 418/1000............. Loss: 0.0071\n",
      "Epoch: 419/1000............. Loss: 0.0071\n",
      "Epoch: 420/1000............. Loss: 0.0067\n",
      "Epoch: 421/1000............. Loss: 0.0067\n",
      "Epoch: 422/1000............. Loss: 0.0064\n",
      "Epoch: 423/1000............. Loss: 0.0067\n",
      "Epoch: 424/1000............. Loss: 0.0068\n",
      "Epoch: 425/1000............. Loss: 0.0076\n",
      "Epoch: 426/1000............. Loss: 0.0070\n",
      "Epoch: 427/1000............. Loss: 0.0066\n",
      "Epoch: 428/1000............. Loss: 0.0066\n",
      "Epoch: 429/1000............. Loss: 0.0067\n",
      "Epoch: 430/1000............. Loss: 0.0065\n",
      "Epoch: 431/1000............. Loss: 0.0067\n",
      "Epoch: 432/1000............. Loss: 0.0071\n",
      "Epoch: 433/1000............. Loss: 0.0065\n",
      "Epoch: 434/1000............. Loss: 0.0070\n",
      "Epoch: 435/1000............. Loss: 0.0068\n",
      "Epoch: 436/1000............. Loss: 0.0068\n",
      "Epoch: 437/1000............. Loss: 0.0067\n",
      "Epoch: 438/1000............. Loss: 0.0065\n",
      "Epoch: 439/1000............. Loss: 0.0069\n",
      "Epoch: 440/1000............. Loss: 0.0063\n",
      "Epoch: 441/1000............. Loss: 0.0070\n",
      "Epoch: 442/1000............. Loss: 0.0069\n",
      "Epoch: 443/1000............. Loss: 0.0067\n",
      "Epoch: 444/1000............. Loss: 0.0068\n",
      "Epoch: 445/1000............. Loss: 0.0065\n",
      "Epoch: 446/1000............. Loss: 0.0067\n",
      "Epoch: 447/1000............. Loss: 0.0069\n",
      "Epoch: 448/1000............. Loss: 0.0066\n",
      "Epoch: 449/1000............. Loss: 0.0070\n",
      "Epoch: 450/1000............. Loss: 0.0063\n",
      "Epoch: 451/1000............. Loss: 0.0066\n",
      "Epoch: 452/1000............. Loss: 0.0065\n",
      "Epoch: 453/1000............. Loss: 0.0065\n",
      "Epoch: 454/1000............. Loss: 0.0068\n",
      "Epoch: 455/1000............. Loss: 0.0065\n",
      "Epoch: 456/1000............. Loss: 0.0068\n",
      "Epoch: 457/1000............. Loss: 0.0070\n",
      "Epoch: 458/1000............. Loss: 0.0067\n",
      "Epoch: 459/1000............. Loss: 0.0066\n",
      "Epoch: 460/1000............. Loss: 0.0068\n",
      "Epoch: 461/1000............. Loss: 0.0065\n",
      "Epoch: 462/1000............. Loss: 0.0075\n",
      "Epoch: 463/1000............. Loss: 0.0065\n",
      "Epoch: 464/1000............. Loss: 0.0065\n",
      "Epoch: 465/1000............. Loss: 0.0068\n",
      "Epoch: 466/1000............. Loss: 0.0067\n",
      "Epoch: 467/1000............. Loss: 0.0064\n",
      "Epoch: 468/1000............. Loss: 0.0066\n",
      "Epoch: 469/1000............. Loss: 0.0075\n",
      "Epoch: 470/1000............. Loss: 0.0065\n",
      "Epoch: 471/1000............. Loss: 0.0067\n",
      "Epoch: 472/1000............. Loss: 0.0068\n",
      "Epoch: 473/1000............. Loss: 0.0064\n",
      "Epoch: 474/1000............. Loss: 0.0065\n",
      "Epoch: 475/1000............. Loss: 0.0065\n",
      "Epoch: 476/1000............. Loss: 0.0067\n",
      "Epoch: 477/1000............. Loss: 0.0063\n",
      "Epoch: 478/1000............. Loss: 0.0066\n",
      "Epoch: 479/1000............. Loss: 0.0067\n",
      "Epoch: 480/1000............. Loss: 0.0068\n",
      "Epoch: 481/1000............. Loss: 0.0072\n",
      "Epoch: 482/1000............. Loss: 0.0068\n",
      "Epoch: 483/1000............. Loss: 0.0068\n",
      "Epoch: 484/1000............. Loss: 0.0065\n",
      "Epoch: 485/1000............. Loss: 0.0078\n",
      "Epoch: 486/1000............. Loss: 0.0065\n",
      "Epoch: 487/1000............. Loss: 0.0068\n",
      "Epoch: 488/1000............. Loss: 0.0067\n",
      "Epoch: 489/1000............. Loss: 0.0067\n",
      "Epoch: 490/1000............. Loss: 0.0067\n",
      "Epoch: 491/1000............. Loss: 0.0066\n",
      "Epoch: 492/1000............. Loss: 0.0063\n",
      "Epoch: 493/1000............. Loss: 0.0069\n",
      "Epoch: 494/1000............. Loss: 0.0064\n",
      "Epoch: 495/1000............. Loss: 0.0064\n",
      "Epoch: 496/1000............. Loss: 0.0068\n",
      "Epoch: 497/1000............. Loss: 0.0067\n",
      "Epoch: 498/1000............. Loss: 0.0067\n",
      "Epoch: 499/1000............. Loss: 0.0066\n",
      "Epoch: 500/1000............. Loss: 0.0065\n",
      "Epoch: 501/1000............. Loss: 0.0065\n",
      "Epoch: 502/1000............. Loss: 0.0064\n",
      "Epoch: 503/1000............. Loss: 0.0063\n",
      "Epoch: 504/1000............. Loss: 0.0065\n",
      "Epoch: 505/1000............. Loss: 0.0064\n",
      "Epoch: 506/1000............. Loss: 0.0065\n",
      "Epoch: 507/1000............. Loss: 0.0066\n",
      "Epoch: 508/1000............. Loss: 0.0067\n",
      "Epoch: 509/1000............. Loss: 0.0066\n",
      "Epoch: 510/1000............. Loss: 0.0065\n",
      "Epoch: 511/1000............. Loss: 0.0067\n",
      "Epoch: 512/1000............. Loss: 0.0065\n",
      "Epoch: 513/1000............. Loss: 0.0066\n",
      "Epoch: 514/1000............. Loss: 0.0065\n",
      "Epoch: 515/1000............. Loss: 0.0071\n",
      "Epoch: 516/1000............. Loss: 0.0064\n",
      "Epoch: 517/1000............. Loss: 0.0065\n",
      "Epoch: 518/1000............. Loss: 0.0067\n",
      "Epoch: 519/1000............. Loss: 0.0069\n",
      "Epoch: 520/1000............. Loss: 0.0077\n",
      "Epoch: 521/1000............. Loss: 0.0066\n",
      "Epoch: 522/1000............. Loss: 0.0065\n",
      "Epoch: 523/1000............. Loss: 0.0063\n",
      "Epoch: 524/1000............. Loss: 0.0067\n",
      "Epoch: 525/1000............. Loss: 0.0063\n",
      "Epoch: 526/1000............. Loss: 0.0064\n",
      "Epoch: 527/1000............. Loss: 0.0068\n",
      "Epoch: 528/1000............. Loss: 0.0063\n",
      "Epoch: 529/1000............. Loss: 0.0066\n",
      "Epoch: 530/1000............. Loss: 0.0062\n",
      "Epoch: 531/1000............. Loss: 0.0063\n",
      "Epoch: 532/1000............. Loss: 0.0068\n",
      "Epoch: 533/1000............. Loss: 0.0065\n",
      "Epoch: 534/1000............. Loss: 0.0063\n",
      "Epoch: 535/1000............. Loss: 0.0063\n",
      "Epoch: 536/1000............. Loss: 0.0076\n",
      "Epoch: 537/1000............. Loss: 0.0069\n",
      "Epoch: 538/1000............. Loss: 0.0063\n",
      "Epoch: 539/1000............. Loss: 0.0066\n",
      "Epoch: 540/1000............. Loss: 0.0063\n",
      "Epoch: 541/1000............. Loss: 0.0065\n",
      "Epoch: 542/1000............. Loss: 0.0064\n",
      "Epoch: 543/1000............. Loss: 0.0064\n",
      "Epoch: 544/1000............. Loss: 0.0068\n",
      "Epoch: 545/1000............. Loss: 0.0061\n",
      "Epoch: 546/1000............. Loss: 0.0067\n",
      "Epoch: 547/1000............. Loss: 0.0066\n",
      "Epoch: 548/1000............. Loss: 0.0065\n",
      "Epoch: 549/1000............. Loss: 0.0068\n",
      "Epoch: 550/1000............. Loss: 0.0068\n",
      "Epoch: 551/1000............. Loss: 0.0065\n",
      "Epoch: 552/1000............. Loss: 0.0062\n",
      "Epoch: 553/1000............. Loss: 0.0065\n",
      "Epoch: 554/1000............. Loss: 0.0067\n",
      "Epoch: 555/1000............. Loss: 0.0063\n",
      "Epoch: 556/1000............. Loss: 0.0064\n",
      "Epoch: 557/1000............. Loss: 0.0067\n",
      "Epoch: 558/1000............. Loss: 0.0067\n",
      "Epoch: 559/1000............. Loss: 0.0062\n",
      "Epoch: 560/1000............. Loss: 0.0061\n",
      "Epoch: 561/1000............. Loss: 0.0066\n",
      "Epoch: 562/1000............. Loss: 0.0061\n",
      "Epoch: 563/1000............. Loss: 0.0066\n",
      "Epoch: 564/1000............. Loss: 0.0063\n",
      "Epoch: 565/1000............. Loss: 0.0063\n",
      "Epoch: 566/1000............. Loss: 0.0064\n",
      "Epoch: 567/1000............. Loss: 0.0071\n",
      "Epoch: 568/1000............. Loss: 0.0062\n",
      "Epoch: 569/1000............. Loss: 0.0062\n",
      "Epoch: 570/1000............. Loss: 0.0064\n",
      "Epoch: 571/1000............. Loss: 0.0064\n",
      "Epoch: 572/1000............. Loss: 0.0062\n",
      "Epoch: 573/1000............. Loss: 0.0063\n",
      "Epoch: 574/1000............. Loss: 0.0063\n",
      "Epoch: 575/1000............. Loss: 0.0062\n",
      "Epoch: 576/1000............. Loss: 0.0066\n",
      "Epoch: 577/1000............. Loss: 0.0062\n",
      "Epoch: 578/1000............. Loss: 0.0063\n",
      "Epoch: 579/1000............. Loss: 0.0067\n",
      "Epoch: 580/1000............. Loss: 0.0062\n",
      "Epoch: 581/1000............. Loss: 0.0061\n",
      "Epoch: 582/1000............. Loss: 0.0062\n",
      "Epoch: 583/1000............. Loss: 0.0063\n",
      "Epoch: 584/1000............. Loss: 0.0063\n",
      "Epoch: 585/1000............. Loss: 0.0060\n",
      "Epoch: 586/1000............. Loss: 0.0066\n",
      "Epoch: 587/1000............. Loss: 0.0064\n",
      "Epoch: 588/1000............. Loss: 0.0063\n",
      "Epoch: 589/1000............. Loss: 0.0064\n",
      "Epoch: 590/1000............. Loss: 0.0062\n",
      "Epoch: 591/1000............. Loss: 0.0060\n",
      "Epoch: 592/1000............. Loss: 0.0065\n",
      "Epoch: 593/1000............. Loss: 0.0061\n",
      "Epoch: 594/1000............. Loss: 0.0063\n",
      "Epoch: 595/1000............. Loss: 0.0064\n",
      "Epoch: 596/1000............. Loss: 0.0064\n",
      "Epoch: 597/1000............. Loss: 0.0065\n",
      "Epoch: 598/1000............. Loss: 0.0063\n",
      "Epoch: 599/1000............. Loss: 0.0063\n",
      "Epoch: 600/1000............. Loss: 0.0063\n",
      "Epoch: 601/1000............. Loss: 0.0064\n",
      "Epoch: 602/1000............. Loss: 0.0061\n",
      "Epoch: 603/1000............. Loss: 0.0060\n",
      "Epoch: 604/1000............. Loss: 0.0064\n",
      "Epoch: 605/1000............. Loss: 0.0061\n",
      "Epoch: 606/1000............. Loss: 0.0061\n",
      "Epoch: 607/1000............. Loss: 0.0064\n",
      "Epoch: 608/1000............. Loss: 0.0066\n",
      "Epoch: 609/1000............. Loss: 0.0060\n",
      "Epoch: 610/1000............. Loss: 0.0061\n",
      "Epoch: 611/1000............. Loss: 0.0071\n",
      "Epoch: 612/1000............. Loss: 0.0063\n",
      "Epoch: 613/1000............. Loss: 0.0061\n",
      "Epoch: 614/1000............. Loss: 0.0062\n",
      "Epoch: 615/1000............. Loss: 0.0067\n",
      "Epoch: 616/1000............. Loss: 0.0060\n",
      "Epoch: 617/1000............. Loss: 0.0064\n",
      "Epoch: 618/1000............. Loss: 0.0061\n",
      "Epoch: 619/1000............. Loss: 0.0061\n",
      "Epoch: 620/1000............. Loss: 0.0066\n",
      "Epoch: 621/1000............. Loss: 0.0063\n",
      "Epoch: 622/1000............. Loss: 0.0060\n",
      "Epoch: 623/1000............. Loss: 0.0063\n",
      "Epoch: 624/1000............. Loss: 0.0060\n",
      "Epoch: 625/1000............. Loss: 0.0070\n",
      "Epoch: 626/1000............. Loss: 0.0063\n",
      "Epoch: 627/1000............. Loss: 0.0065\n",
      "Epoch: 628/1000............. Loss: 0.0064\n",
      "Epoch: 629/1000............. Loss: 0.0061\n",
      "Epoch: 630/1000............. Loss: 0.0059\n",
      "Epoch: 631/1000............. Loss: 0.0061\n",
      "Epoch: 632/1000............. Loss: 0.0066\n",
      "Epoch: 633/1000............. Loss: 0.0060\n",
      "Epoch: 634/1000............. Loss: 0.0061\n",
      "Epoch: 635/1000............. Loss: 0.0061\n",
      "Epoch: 636/1000............. Loss: 0.0059\n",
      "Epoch: 637/1000............. Loss: 0.0064\n",
      "Epoch: 638/1000............. Loss: 0.0063\n",
      "Epoch: 639/1000............. Loss: 0.0059\n",
      "Epoch: 640/1000............. Loss: 0.0060\n",
      "Epoch: 641/1000............. Loss: 0.0062\n",
      "Epoch: 642/1000............. Loss: 0.0060\n",
      "Epoch: 643/1000............. Loss: 0.0062\n",
      "Epoch: 644/1000............. Loss: 0.0061\n",
      "Epoch: 645/1000............. Loss: 0.0062\n",
      "Epoch: 646/1000............. Loss: 0.0061\n",
      "Epoch: 647/1000............. Loss: 0.0064\n",
      "Epoch: 648/1000............. Loss: 0.0060\n",
      "Epoch: 649/1000............. Loss: 0.0068\n",
      "Epoch: 650/1000............. Loss: 0.0059\n",
      "Epoch: 651/1000............. Loss: 0.0061\n",
      "Epoch: 652/1000............. Loss: 0.0061\n",
      "Epoch: 653/1000............. Loss: 0.0060\n",
      "Epoch: 654/1000............. Loss: 0.0060\n",
      "Epoch: 655/1000............. Loss: 0.0058\n",
      "Epoch: 656/1000............. Loss: 0.0060\n",
      "Epoch: 657/1000............. Loss: 0.0061\n",
      "Epoch: 658/1000............. Loss: 0.0061\n",
      "Epoch: 659/1000............. Loss: 0.0059\n",
      "Epoch: 660/1000............. Loss: 0.0065\n",
      "Epoch: 661/1000............. Loss: 0.0062\n",
      "Epoch: 662/1000............. Loss: 0.0061\n",
      "Epoch: 663/1000............. Loss: 0.0059\n",
      "Epoch: 664/1000............. Loss: 0.0057\n",
      "Epoch: 665/1000............. Loss: 0.0061\n",
      "Epoch: 666/1000............. Loss: 0.0059\n",
      "Epoch: 667/1000............. Loss: 0.0058\n",
      "Epoch: 668/1000............. Loss: 0.0060\n",
      "Epoch: 669/1000............. Loss: 0.0059\n",
      "Epoch: 670/1000............. Loss: 0.0061\n",
      "Epoch: 671/1000............. Loss: 0.0064\n",
      "Epoch: 672/1000............. Loss: 0.0063\n",
      "Epoch: 673/1000............. Loss: 0.0061\n",
      "Epoch: 674/1000............. Loss: 0.0060\n",
      "Epoch: 675/1000............. Loss: 0.0062\n",
      "Epoch: 676/1000............. Loss: 0.0061\n",
      "Epoch: 677/1000............. Loss: 0.0059\n",
      "Epoch: 678/1000............. Loss: 0.0065\n",
      "Epoch: 679/1000............. Loss: 0.0058\n",
      "Epoch: 680/1000............. Loss: 0.0063\n",
      "Epoch: 681/1000............. Loss: 0.0060\n",
      "Epoch: 682/1000............. Loss: 0.0065\n",
      "Epoch: 683/1000............. Loss: 0.0066\n",
      "Epoch: 684/1000............. Loss: 0.0064\n",
      "Epoch: 685/1000............. Loss: 0.0061\n",
      "Epoch: 686/1000............. Loss: 0.0058\n",
      "Epoch: 687/1000............. Loss: 0.0062\n",
      "Epoch: 688/1000............. Loss: 0.0057\n",
      "Epoch: 689/1000............. Loss: 0.0062\n",
      "Epoch: 690/1000............. Loss: 0.0063\n",
      "Epoch: 691/1000............. Loss: 0.0059\n",
      "Epoch: 692/1000............. Loss: 0.0061\n",
      "Epoch: 693/1000............. Loss: 0.0060\n",
      "Epoch: 694/1000............. Loss: 0.0061\n",
      "Epoch: 695/1000............. Loss: 0.0058\n",
      "Epoch: 696/1000............. Loss: 0.0060\n",
      "Epoch: 697/1000............. Loss: 0.0059\n",
      "Epoch: 698/1000............. Loss: 0.0060\n",
      "Epoch: 699/1000............. Loss: 0.0060\n",
      "Epoch: 700/1000............. Loss: 0.0063\n",
      "Epoch: 701/1000............. Loss: 0.0061\n",
      "Epoch: 702/1000............. Loss: 0.0059\n",
      "Epoch: 703/1000............. Loss: 0.0060\n",
      "Epoch: 704/1000............. Loss: 0.0060\n",
      "Epoch: 705/1000............. Loss: 0.0061\n",
      "Epoch: 706/1000............. Loss: 0.0055\n",
      "Epoch: 707/1000............. Loss: 0.0057\n",
      "Epoch: 708/1000............. Loss: 0.0056\n",
      "Epoch: 709/1000............. Loss: 0.0057\n",
      "Epoch: 710/1000............. Loss: 0.0060\n",
      "Epoch: 711/1000............. Loss: 0.0061\n",
      "Epoch: 712/1000............. Loss: 0.0059\n",
      "Epoch: 713/1000............. Loss: 0.0058\n",
      "Epoch: 714/1000............. Loss: 0.0059\n",
      "Epoch: 715/1000............. Loss: 0.0058\n",
      "Epoch: 716/1000............. Loss: 0.0058\n",
      "Epoch: 717/1000............. Loss: 0.0060\n",
      "Epoch: 718/1000............. Loss: 0.0059\n",
      "Epoch: 719/1000............. Loss: 0.0058\n",
      "Epoch: 720/1000............. Loss: 0.0057\n",
      "Epoch: 721/1000............. Loss: 0.0062\n",
      "Epoch: 722/1000............. Loss: 0.0059\n",
      "Epoch: 723/1000............. Loss: 0.0058\n",
      "Epoch: 724/1000............. Loss: 0.0060\n",
      "Epoch: 725/1000............. Loss: 0.0059\n",
      "Epoch: 726/1000............. Loss: 0.0060\n",
      "Epoch: 727/1000............. Loss: 0.0059\n",
      "Epoch: 728/1000............. Loss: 0.0057\n",
      "Epoch: 729/1000............. Loss: 0.0056\n",
      "Epoch: 730/1000............. Loss: 0.0057\n",
      "Epoch: 731/1000............. Loss: 0.0057\n",
      "Epoch: 732/1000............. Loss: 0.0057\n",
      "Epoch: 733/1000............. Loss: 0.0058\n",
      "Epoch: 734/1000............. Loss: 0.0057\n",
      "Epoch: 735/1000............. Loss: 0.0058\n",
      "Epoch: 736/1000............. Loss: 0.0062\n",
      "Epoch: 737/1000............. Loss: 0.0061\n",
      "Epoch: 738/1000............. Loss: 0.0056\n",
      "Epoch: 739/1000............. Loss: 0.0057\n",
      "Epoch: 740/1000............. Loss: 0.0058\n",
      "Epoch: 741/1000............. Loss: 0.0057\n",
      "Epoch: 742/1000............. Loss: 0.0059\n",
      "Epoch: 743/1000............. Loss: 0.0058\n",
      "Epoch: 744/1000............. Loss: 0.0056\n",
      "Epoch: 745/1000............. Loss: 0.0056\n",
      "Epoch: 746/1000............. Loss: 0.0058\n",
      "Epoch: 747/1000............. Loss: 0.0059\n",
      "Epoch: 748/1000............. Loss: 0.0056\n",
      "Epoch: 749/1000............. Loss: 0.0060\n",
      "Epoch: 750/1000............. Loss: 0.0057\n",
      "Epoch: 751/1000............. Loss: 0.0061\n",
      "Epoch: 752/1000............. Loss: 0.0057\n",
      "Epoch: 753/1000............. Loss: 0.0059\n",
      "Epoch: 754/1000............. Loss: 0.0060\n",
      "Epoch: 755/1000............. Loss: 0.0055\n",
      "Epoch: 756/1000............. Loss: 0.0057\n",
      "Epoch: 757/1000............. Loss: 0.0060\n",
      "Epoch: 758/1000............. Loss: 0.0055\n",
      "Epoch: 759/1000............. Loss: 0.0065\n",
      "Epoch: 760/1000............. Loss: 0.0057\n",
      "Epoch: 761/1000............. Loss: 0.0057\n",
      "Epoch: 762/1000............. Loss: 0.0055\n",
      "Epoch: 763/1000............. Loss: 0.0056\n",
      "Epoch: 764/1000............. Loss: 0.0057\n",
      "Epoch: 765/1000............. Loss: 0.0059\n",
      "Epoch: 766/1000............. Loss: 0.0054\n",
      "Epoch: 767/1000............. Loss: 0.0057\n",
      "Epoch: 768/1000............. Loss: 0.0060\n",
      "Epoch: 769/1000............. Loss: 0.0057\n",
      "Epoch: 770/1000............. Loss: 0.0059\n",
      "Epoch: 771/1000............. Loss: 0.0058\n",
      "Epoch: 772/1000............. Loss: 0.0059\n",
      "Epoch: 773/1000............. Loss: 0.0059\n",
      "Epoch: 774/1000............. Loss: 0.0058\n",
      "Epoch: 775/1000............. Loss: 0.0056\n",
      "Epoch: 776/1000............. Loss: 0.0057\n",
      "Epoch: 777/1000............. Loss: 0.0058\n",
      "Epoch: 778/1000............. Loss: 0.0060\n",
      "Epoch: 779/1000............. Loss: 0.0058\n",
      "Epoch: 780/1000............. Loss: 0.0054\n",
      "Epoch: 781/1000............. Loss: 0.0061\n",
      "Epoch: 782/1000............. Loss: 0.0055\n",
      "Epoch: 783/1000............. Loss: 0.0058\n",
      "Epoch: 784/1000............. Loss: 0.0057\n",
      "Epoch: 785/1000............. Loss: 0.0056\n",
      "Epoch: 786/1000............. Loss: 0.0056\n",
      "Epoch: 787/1000............. Loss: 0.0055\n",
      "Epoch: 788/1000............. Loss: 0.0054\n",
      "Epoch: 789/1000............. Loss: 0.0055\n",
      "Epoch: 790/1000............. Loss: 0.0055\n",
      "Epoch: 791/1000............. Loss: 0.0055\n",
      "Epoch: 792/1000............. Loss: 0.0056\n",
      "Epoch: 793/1000............. Loss: 0.0058\n",
      "Epoch: 794/1000............. Loss: 0.0057\n",
      "Epoch: 795/1000............. Loss: 0.0057\n",
      "Epoch: 796/1000............. Loss: 0.0058\n",
      "Epoch: 797/1000............. Loss: 0.0058\n",
      "Epoch: 798/1000............. Loss: 0.0054\n",
      "Epoch: 799/1000............. Loss: 0.0057\n",
      "Epoch: 800/1000............. Loss: 0.0055\n",
      "Epoch: 801/1000............. Loss: 0.0057\n",
      "Epoch: 802/1000............. Loss: 0.0057\n",
      "Epoch: 803/1000............. Loss: 0.0053\n",
      "Epoch: 804/1000............. Loss: 0.0056\n",
      "Epoch: 805/1000............. Loss: 0.0056\n",
      "Epoch: 806/1000............. Loss: 0.0056\n",
      "Epoch: 807/1000............. Loss: 0.0052\n",
      "Epoch: 808/1000............. Loss: 0.0058\n",
      "Epoch: 809/1000............. Loss: 0.0057\n",
      "Epoch: 810/1000............. Loss: 0.0054\n",
      "Epoch: 811/1000............. Loss: 0.0058\n",
      "Epoch: 812/1000............. Loss: 0.0057\n",
      "Epoch: 813/1000............. Loss: 0.0053\n",
      "Epoch: 814/1000............. Loss: 0.0057\n",
      "Epoch: 815/1000............. Loss: 0.0058\n",
      "Epoch: 816/1000............. Loss: 0.0056\n",
      "Epoch: 817/1000............. Loss: 0.0057\n",
      "Epoch: 818/1000............. Loss: 0.0057\n",
      "Epoch: 819/1000............. Loss: 0.0055\n",
      "Epoch: 820/1000............. Loss: 0.0062\n",
      "Epoch: 821/1000............. Loss: 0.0055\n",
      "Epoch: 822/1000............. Loss: 0.0054\n",
      "Epoch: 823/1000............. Loss: 0.0055\n",
      "Epoch: 824/1000............. Loss: 0.0055\n",
      "Epoch: 825/1000............. Loss: 0.0058\n",
      "Epoch: 826/1000............. Loss: 0.0056\n",
      "Epoch: 827/1000............. Loss: 0.0055\n",
      "Epoch: 828/1000............. Loss: 0.0060\n",
      "Epoch: 829/1000............. Loss: 0.0053\n",
      "Epoch: 830/1000............. Loss: 0.0053\n",
      "Epoch: 831/1000............. Loss: 0.0054\n",
      "Epoch: 832/1000............. Loss: 0.0054\n",
      "Epoch: 833/1000............. Loss: 0.0055\n",
      "Epoch: 834/1000............. Loss: 0.0054\n",
      "Epoch: 835/1000............. Loss: 0.0059\n",
      "Epoch: 836/1000............. Loss: 0.0056\n",
      "Epoch: 837/1000............. Loss: 0.0055\n",
      "Epoch: 838/1000............. Loss: 0.0053\n",
      "Epoch: 839/1000............. Loss: 0.0056\n",
      "Epoch: 840/1000............. Loss: 0.0054\n",
      "Epoch: 841/1000............. Loss: 0.0053\n",
      "Epoch: 842/1000............. Loss: 0.0057\n",
      "Epoch: 843/1000............. Loss: 0.0053\n",
      "Epoch: 844/1000............. Loss: 0.0055\n",
      "Epoch: 845/1000............. Loss: 0.0055\n",
      "Epoch: 846/1000............. Loss: 0.0053\n",
      "Epoch: 847/1000............. Loss: 0.0056\n",
      "Epoch: 848/1000............. Loss: 0.0055\n",
      "Epoch: 849/1000............. Loss: 0.0058\n",
      "Epoch: 850/1000............. Loss: 0.0054\n",
      "Epoch: 851/1000............. Loss: 0.0054\n",
      "Epoch: 852/1000............. Loss: 0.0053\n",
      "Epoch: 853/1000............. Loss: 0.0058\n",
      "Epoch: 854/1000............. Loss: 0.0054\n",
      "Epoch: 855/1000............. Loss: 0.0057\n",
      "Epoch: 856/1000............. Loss: 0.0054\n",
      "Epoch: 857/1000............. Loss: 0.0053\n",
      "Epoch: 858/1000............. Loss: 0.0054\n",
      "Epoch: 859/1000............. Loss: 0.0053\n",
      "Epoch: 860/1000............. Loss: 0.0055\n",
      "Epoch: 861/1000............. Loss: 0.0052\n",
      "Epoch: 862/1000............. Loss: 0.0054\n",
      "Epoch: 863/1000............. Loss: 0.0054\n",
      "Epoch: 864/1000............. Loss: 0.0053\n",
      "Epoch: 865/1000............. Loss: 0.0053\n",
      "Epoch: 866/1000............. Loss: 0.0055\n",
      "Epoch: 867/1000............. Loss: 0.0057\n",
      "Epoch: 868/1000............. Loss: 0.0058\n",
      "Epoch: 869/1000............. Loss: 0.0055\n",
      "Epoch: 870/1000............. Loss: 0.0053\n",
      "Epoch: 871/1000............. Loss: 0.0053\n",
      "Epoch: 872/1000............. Loss: 0.0054\n",
      "Epoch: 873/1000............. Loss: 0.0055\n",
      "Epoch: 874/1000............. Loss: 0.0053\n",
      "Epoch: 875/1000............. Loss: 0.0051\n",
      "Epoch: 876/1000............. Loss: 0.0055\n",
      "Epoch: 877/1000............. Loss: 0.0055\n",
      "Epoch: 878/1000............. Loss: 0.0053\n",
      "Epoch: 879/1000............. Loss: 0.0054\n",
      "Epoch: 880/1000............. Loss: 0.0055\n",
      "Epoch: 881/1000............. Loss: 0.0056\n",
      "Epoch: 882/1000............. Loss: 0.0054\n",
      "Epoch: 883/1000............. Loss: 0.0055\n",
      "Epoch: 884/1000............. Loss: 0.0050\n",
      "Epoch: 885/1000............. Loss: 0.0055\n",
      "Epoch: 886/1000............. Loss: 0.0055\n",
      "Epoch: 887/1000............. Loss: 0.0053\n",
      "Epoch: 888/1000............. Loss: 0.0050\n",
      "Epoch: 889/1000............. Loss: 0.0051\n",
      "Epoch: 890/1000............. Loss: 0.0054\n",
      "Epoch: 891/1000............. Loss: 0.0055\n",
      "Epoch: 892/1000............. Loss: 0.0052\n",
      "Epoch: 893/1000............. Loss: 0.0054\n",
      "Epoch: 894/1000............. Loss: 0.0054\n",
      "Epoch: 895/1000............. Loss: 0.0053\n",
      "Epoch: 896/1000............. Loss: 0.0055\n",
      "Epoch: 897/1000............. Loss: 0.0053\n",
      "Epoch: 898/1000............. Loss: 0.0051\n",
      "Epoch: 899/1000............. Loss: 0.0052\n",
      "Epoch: 900/1000............. Loss: 0.0051\n",
      "Epoch: 901/1000............. Loss: 0.0055\n",
      "Epoch: 902/1000............. Loss: 0.0055\n",
      "Epoch: 903/1000............. Loss: 0.0053\n",
      "Epoch: 904/1000............. Loss: 0.0050\n",
      "Epoch: 905/1000............. Loss: 0.0053\n",
      "Epoch: 906/1000............. Loss: 0.0054\n",
      "Epoch: 907/1000............. Loss: 0.0052\n",
      "Epoch: 908/1000............. Loss: 0.0055\n",
      "Epoch: 909/1000............. Loss: 0.0053\n",
      "Epoch: 910/1000............. Loss: 0.0055\n",
      "Epoch: 911/1000............. Loss: 0.0052\n",
      "Epoch: 912/1000............. Loss: 0.0056\n",
      "Epoch: 913/1000............. Loss: 0.0053\n",
      "Epoch: 914/1000............. Loss: 0.0053\n",
      "Epoch: 915/1000............. Loss: 0.0050\n",
      "Epoch: 916/1000............. Loss: 0.0051\n",
      "Epoch: 917/1000............. Loss: 0.0051\n",
      "Epoch: 918/1000............. Loss: 0.0051\n",
      "Epoch: 919/1000............. Loss: 0.0053\n",
      "Epoch: 920/1000............. Loss: 0.0052\n",
      "Epoch: 921/1000............. Loss: 0.0053\n",
      "Epoch: 922/1000............. Loss: 0.0057\n",
      "Epoch: 923/1000............. Loss: 0.0053\n",
      "Epoch: 924/1000............. Loss: 0.0053\n",
      "Epoch: 925/1000............. Loss: 0.0050\n",
      "Epoch: 926/1000............. Loss: 0.0052\n",
      "Epoch: 927/1000............. Loss: 0.0050\n",
      "Epoch: 928/1000............. Loss: 0.0051\n",
      "Epoch: 929/1000............. Loss: 0.0055\n",
      "Epoch: 930/1000............. Loss: 0.0052\n",
      "Epoch: 931/1000............. Loss: 0.0053\n",
      "Epoch: 932/1000............. Loss: 0.0052\n",
      "Epoch: 933/1000............. Loss: 0.0054\n",
      "Epoch: 934/1000............. Loss: 0.0054\n",
      "Epoch: 935/1000............. Loss: 0.0053\n",
      "Epoch: 936/1000............. Loss: 0.0051\n",
      "Epoch: 937/1000............. Loss: 0.0053\n",
      "Epoch: 938/1000............. Loss: 0.0052\n",
      "Epoch: 939/1000............. Loss: 0.0051\n",
      "Epoch: 940/1000............. Loss: 0.0052\n",
      "Epoch: 941/1000............. Loss: 0.0052\n",
      "Epoch: 942/1000............. Loss: 0.0055\n",
      "Epoch: 943/1000............. Loss: 0.0052\n",
      "Epoch: 944/1000............. Loss: 0.0051\n",
      "Epoch: 945/1000............. Loss: 0.0051\n",
      "Epoch: 946/1000............. Loss: 0.0054\n",
      "Epoch: 947/1000............. Loss: 0.0053\n",
      "Epoch: 948/1000............. Loss: 0.0050\n",
      "Epoch: 949/1000............. Loss: 0.0052\n",
      "Epoch: 950/1000............. Loss: 0.0052\n",
      "Epoch: 951/1000............. Loss: 0.0054\n",
      "Epoch: 952/1000............. Loss: 0.0053\n",
      "Epoch: 953/1000............. Loss: 0.0051\n",
      "Epoch: 954/1000............. Loss: 0.0052\n",
      "Epoch: 955/1000............. Loss: 0.0053\n",
      "Epoch: 956/1000............. Loss: 0.0050\n",
      "Epoch: 957/1000............. Loss: 0.0049\n",
      "Epoch: 958/1000............. Loss: 0.0053\n",
      "Epoch: 959/1000............. Loss: 0.0052\n",
      "Epoch: 960/1000............. Loss: 0.0050\n",
      "Epoch: 961/1000............. Loss: 0.0054\n",
      "Epoch: 962/1000............. Loss: 0.0052\n",
      "Epoch: 963/1000............. Loss: 0.0051\n",
      "Epoch: 964/1000............. Loss: 0.0051\n",
      "Epoch: 965/1000............. Loss: 0.0052\n",
      "Epoch: 966/1000............. Loss: 0.0049\n",
      "Epoch: 967/1000............. Loss: 0.0053\n",
      "Epoch: 968/1000............. Loss: 0.0051\n",
      "Epoch: 969/1000............. Loss: 0.0051\n",
      "Epoch: 970/1000............. Loss: 0.0052\n",
      "Epoch: 971/1000............. Loss: 0.0052\n",
      "Epoch: 972/1000............. Loss: 0.0050\n",
      "Epoch: 973/1000............. Loss: 0.0050\n",
      "Epoch: 974/1000............. Loss: 0.0049\n",
      "Epoch: 975/1000............. Loss: 0.0050\n",
      "Epoch: 976/1000............. Loss: 0.0052\n",
      "Epoch: 977/1000............. Loss: 0.0051\n",
      "Epoch: 978/1000............. Loss: 0.0052\n",
      "Epoch: 979/1000............. Loss: 0.0055\n",
      "Epoch: 980/1000............. Loss: 0.0050\n",
      "Epoch: 981/1000............. Loss: 0.0048\n",
      "Epoch: 982/1000............. Loss: 0.0053\n",
      "Epoch: 983/1000............. Loss: 0.0051\n",
      "Epoch: 984/1000............. Loss: 0.0050\n",
      "Epoch: 985/1000............. Loss: 0.0051\n",
      "Epoch: 986/1000............. Loss: 0.0051\n",
      "Epoch: 987/1000............. Loss: 0.0050\n",
      "Epoch: 988/1000............. Loss: 0.0050\n",
      "Epoch: 989/1000............. Loss: 0.0049\n",
      "Epoch: 990/1000............. Loss: 0.0051\n",
      "Epoch: 991/1000............. Loss: 0.0050\n",
      "Epoch: 992/1000............. Loss: 0.0051\n",
      "Epoch: 993/1000............. Loss: 0.0050\n",
      "Epoch: 994/1000............. Loss: 0.0047\n",
      "Epoch: 995/1000............. Loss: 0.0050\n",
      "Epoch: 996/1000............. Loss: 0.0047\n",
      "Epoch: 997/1000............. Loss: 0.0052\n",
      "Epoch: 998/1000............. Loss: 0.0048\n",
      "Epoch: 999/1000............. Loss: 0.0050\n",
      "Epoch: 1000/1000............. Loss: 0.0050\n"
     ]
    }
   ],
   "source": [
    "mile_stone = []\n",
    "for i in range(1, 10):\n",
    "    mile_stone.append((num_epochs * i)//20)\n",
    "stepping_rate = 0.95\n",
    "\n",
    "train_weather_transformer(model, num_epochs, train_iter, combined_loss, device, lr, mile_stone, stepping_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('test_weather_predicting.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:57:35.319312Z",
     "end_time": "2023-04-17T04:57:35.380387Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "test_iter = create_weather_dataset('test_data1.csv', 72, 24, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:57:37.395835Z",
     "end_time": "2023-04-17T04:57:37.457896Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def predict_weather(model, X_data, X_weather, X_valid_len, pred_step_len):\n",
    "    model.eval()\n",
    "    if len(X_data.shape) < 3:\n",
    "        X_data = X_data.unsqueeze(0)\n",
    "        X_weather = X_weather.unsqueeze(0)\n",
    "        X_valid_len = X_valid_len.unsqueeze(0)\n",
    "\n",
    "    X_data = X_data.to(device)\n",
    "    X_weather = X_weather.to(device)\n",
    "    X_valid_len = X_valid_len.to(device)\n",
    "    X = (X_data, X_weather)\n",
    "    enc_outputs = model.encoder(X, X_valid_len)\n",
    "    dec_state = model.decoder.init_state(enc_outputs, X_valid_len)\n",
    "    bos_data = X_data[:, -1, :].unsqueeze(1)\n",
    "    bos_weather = X_weather[:, -1].unsqueeze(1)\n",
    "    dec_input = (bos_data, bos_weather)\n",
    "    output_seq = []\n",
    "    for _ in range(pred_step_len):\n",
    "        Y, dec_state = model.decoder(dec_input, dec_state)\n",
    "        Y_data = Y[:, :, :num_data_dim]\n",
    "        Y_weather = Y[:, :, num_data_dim:].argmax(dim=2)\n",
    "        dec_input = (Y_data, Y_weather)\n",
    "        output_seq.append(dec_input)\n",
    "    return output_seq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:57:38.857479Z",
     "end_time": "2023-04-17T04:57:38.904030Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "test_X_data, test_X_weather, test_X_valid_len, test_Y_data, test_Y_weather, test_Y_valid_len = [x.to(device) for x in next(iter(test_iter))]\n",
    "output_seq = predict_weather(model, test_X_data[0], test_X_weather[0], test_X_valid_len[0], 24)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:57:40.708609Z",
     "end_time": "2023-04-17T04:57:46.565523Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 9.4206e+00, -1.3570e+00,  5.0197e+01,  6.7593e-04,  2.3973e+01,\n           1.0027e+02,  1.6451e+01]]], device='cuda:0',\n       grad_fn=<SliceBackward0>)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:57:48.276572Z",
     "end_time": "2023-04-17T04:57:48.307105Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def test_model_performance(model, data_iter, pred_step_len):\n",
    "    model.eval()\n",
    "    metric = Accumulator(2)\n",
    "    for batch in data_iter:\n",
    "\n",
    "        X_data, X_weather, X_valid_len, Y_data, Y_weather, Y_valid_len = [x.to(device) for x in batch]\n",
    "        X_data = X_data.to(device)\n",
    "        X_weather = X_weather.to(device)\n",
    "        X_valid_len = X_valid_len.to(device)\n",
    "        X = (X_data, X_weather)\n",
    "        enc_outputs = model.encoder(X, X_valid_len)\n",
    "        dec_state = model.decoder.init_state(enc_outputs, X_valid_len)\n",
    "        bos_data = X_data[:, -1, :].unsqueeze(1)\n",
    "        bos_weather = X_weather[:, -1].unsqueeze(1)\n",
    "        dec_input = (bos_data, bos_weather)\n",
    "        output_weathers = torch.zeros((X_data.shape[0], pred_step_len, num_data_dim+num_weather), device=device)\n",
    "        for i in range(pred_step_len):\n",
    "            Y_hat, dec_state = model.decoder(dec_input, dec_state)\n",
    "            output_weathers[:, i, :] = Y_hat.squeeze(1)\n",
    "            dec_input = (Y_hat[:, :, :num_data_dim], Y_hat[:, :, num_data_dim:].argmax(dim=2))\n",
    "        l = combined_loss(output_weathers, Y_data, Y_weather)\n",
    "        num_tokens = Y_valid_len.sum()\n",
    "        metric.add(l.sum(), num_tokens)\n",
    "    print(\"Loss: {:.4f}\".format((metric[0] / metric[1])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:33:10.783121Z",
     "end_time": "2023-04-17T04:33:10.811643Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0468\n"
     ]
    }
   ],
   "source": [
    "test_model_performance(model, test_iter, 24)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:33:13.271931Z",
     "end_time": "2023-04-17T04:33:25.919251Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
